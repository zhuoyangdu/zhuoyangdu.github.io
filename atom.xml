<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhuoyang Du</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhuoyangdu.github.io/"/>
  <updated>2018-02-01T09:32:46.765Z</updated>
  <id>https://zhuoyangdu.github.io/</id>
  
  <author>
    <name>Zhuoyang Du</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Building deep neural network - step by step</title>
    <link href="https://zhuoyangdu.github.io/2018/02/01/build-deep-neural-network/"/>
    <id>https://zhuoyangdu.github.io/2018/02/01/build-deep-neural-network/</id>
    <published>2018-02-01T08:43:03.000Z</published>
    <updated>2018-02-01T09:32:46.765Z</updated>
    
    <content type="html"><![CDATA[<p>源链接：<a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="external">https://www.coursera.org/learn/neural-networks-deep-learning/</a> Week4 assignment(part 1 of 2)</p><ul><li>这周内容：实现所有建立深度神经网络需要的函数</li><li>下周内容：构建用于图像分类的深度神经网络</li></ul><p><strong> 本次作业的要求是</strong></p><ul><li>用非线性单元（如：ReLU）来改进模型</li><li>建立深度神经网络</li><li>实现易于实用的神经网络</li></ul><p><strong>符号标记</strong></p><ul><li>上标 $[l]$ 表示与第$l$层相关的量<ul><li>例：$a^{[L]}$是第$L$层激活层，$W^{[L]}$和$b^{[L]}$分别是第$L$层的参数。</li></ul></li><li>上标$(i)$表示与第$i$个样本相关的量<ul><li>例：$x^(i)$是第$i$个训练样本</li></ul></li><li>下标$i$表示某个向量的第$i$个量<ul><li>例：$a^{[l]}_i$表示第$l$个激活层的第$i$个元素</li></ul></li></ul><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>首先导入你需要用到的所有python库。</p><ul><li><code>numpy</code></li><li><code>matplotlib</code></li><li><code>dnn_utils</code>：提供了本文需要用到的一些必要的函数</li><li><code>testCases</code>：提供了一些测试实例</li><li><code>np.random.seed(1)</code>用来保证所有的随机函数都是一致的。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> h5py</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></div><div class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></div><div class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></div><div class="line"></div><div class="line">%load_ext autoreload</div><div class="line">%autoreload <span class="number">2</span></div><div class="line"></div><div class="line">np.random.seed(<span class="number">1</span>)</div></pre></td></tr></table></figure><h2 id="Outline-of-the-Assignment"><a href="#Outline-of-the-Assignment" class="headerlink" title="Outline of the Assignment"></a>Outline of the Assignment</h2><ul><li>初始化两层神经网络和$L$层神经网络的参数</li><li>实现前向传播模块（forward propagation module）<ul><li>实现某一层前向传播的线性部分（LINEAR）（结果为$Z^{[l]}$）</li><li>给定激活函数ACTIVATION（relu/sigmoid）</li><li>结合前两步实现新的[LINEAR-&gt;ACTIVATION]前向函数</li><li>堆叠[LINEAR-&gt;ACTIVATION]前向函数$L-1$次（从第1层到第$L-1$层，在结尾加上[LINEAR-&gt;SIGMOID]作为最后第$L$层），则得到新的函数 L_model_forward。</li></ul></li><li>计算损失</li><li>实现后向传播模块<ul><li>计算某一层后向传播的线性部分LINEAR。</li><li>给定激活函数ACTIVATION的梯度（relu_backward/sigmoid_backward）。</li><li>结合前面两步得到新的后向函数[LINEAR-&gt;ACTIVATION]。</li><li>堆叠[LINEAR-&gt;ACTIVATION]后向函数$L-1$次，并加入[LINEAR-&gt;SIGMOID]后向函数，得到新的函数 L_model_backward。</li></ul></li><li>最后更新参数。</li></ul><h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>下文实现了两个初始化的函数，第一个是用来初始化两层神经网络，第二个是$L$层神经网络。</p><h3 id="2-lay-Neural-Network"><a href="#2-lay-Neural-Network" class="headerlink" title="2-lay Neural Network"></a>2-lay Neural Network</h3><p><strong>Exercise</strong>: 创建并初始化二层神经网络</p><p><strong>Instructions</strong>: </p><ul><li>模型结构是LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</li><li>权重矩阵随机初始化，用：<code>np.random.randn(shape)*0.01</code></li><li>偏差初始化为0: <code>mp.zeros(shape)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Argument:</span></div><div class="line"><span class="string">    n_x -- size of the input layer</span></div><div class="line"><span class="string">    n_h -- size of the hidden layer</span></div><div class="line"><span class="string">    n_y -- size of the output layer</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></div><div class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></div><div class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></div><div class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></div><div class="line">    b1 = np.zeros(shape=(n_h, <span class="number">1</span>))</div><div class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></div><div class="line">    b2 = np.zeros(shape=(n_y, <span class="number">1</span>))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</div><div class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</div><div class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</div><div class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</div><div class="line">    </div><div class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</div><div class="line">                  <span class="string">"b1"</span>: b1,</div><div class="line">                  <span class="string">"W2"</span>: W2,</div><div class="line">                  <span class="string">"b2"</span>: b2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">parameters = initialize_parameters(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure><p><strong>Expected output</strong>:</p><table><thead><tr><th>name</th><th>value</th></tr></thead><tbody><tr><td><strong>W1</strong></td><td>[[ 0.01624345 -0.00611756] [-0.00528172 -0.01072969]]</td></tr><tr><td><strong>W2</strong></td><td>[[ 0.00865408 -0.02301539]]</td></tr><tr><td><strong>b1</strong></td><td>[[ 0.] [ 0.]]</td></tr><tr><td><strong>b2</strong></td><td>[[ 0.]]</td></tr></tbody></table><h3 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h3><p>L层神经网络的初始化要相对复杂一些。初始化的时候要注意匹配每一层的维度，$n^{[l]}$表示第$l$层的神经元数量，如果输入$X$是$(12288,209)$（有209个训练样本），则：</p><table><thead><tr><th></th><th>shape of W</th><th>shape of b</th><th>Activation</th><th>Shape of Activation</th></tr></thead><tbody><tr><td>Layer1</td><td>$(n^{[1]},12288)$</td><td>$(n^{[1]},1)$</td><td>$Z^{[1]}=W^{[1]}X+b^{[1]}$</td><td>$(n^{[1]},209)$</td></tr><tr><td>Layer2</td><td>$(n^{[2]},n^{[1]})$</td><td>$(n^{[2]},1)$</td><td>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</td><td>$(n^{[2]},209)$</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>Layer L-1</td><td>$(n^{[L-1]},n^{[L-2]})$</td><td>$(n^{[L-1]},1)$</td><td>$Z^{[L-1]}=W^{[L-1]}A^{[L-2]}+b^{[L-1]}$</td><td>$(n^{[L-1]},209)$</td></tr><tr><td>Layer L</td><td>$(n^{[L]},n^{[L-1]})$</td><td>$(n^{[L]},1)$</td><td>$Z^{[L]}=W^{[L]}A^{[L-1]}+b^{[L]}$</td><td>$(n^{[L]},209)$</td></tr></tbody></table><p>需要注意的是，由于python的<strong>broadcast</strong>特性，如果计算 <code>WX+b</code>，最后得到的结果是：<br>$$W = \begin{bmatrix} j &amp; k &amp; l \\ m &amp; n &amp; o \\ p &amp; q &amp; r \end{bmatrix},<br> X =  \begin{bmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{bmatrix},<br> b = \begin{bmatrix} s \\ t \\ u \end{bmatrix}$$<br>$$WX+b = \begin{bmatrix} (ja+kd+lg)+s &amp; (jb+ke+lh)+s &amp; (jc+kf+li)+s \\<br>(ma+nd+og)+t &amp; (mb+ne+oh)+t &amp; (mc+nf+oi)+t \\<br>(pa+qd+rg)+u &amp; (pb+qe+rh)+u &amp; (pc+qf+ri)+u\end{bmatrix}$$</p><p><strong>Excercise</strong> 实现L层神经网络的初始化</p><p><strong>Instructions</strong>:</p><ul><li>模型结构是[LINEAR-&gt;RELU] × (L-1)-&gt;LINEAR-&gt;SIGMOID. 由L-1层RELU激活函数和带有sigmoid函数的输出层构成。</li><li>权重矩阵随机初始化，采用<code>np.random.rand(shape)*0.01</code></li><li>偏差向量零初始化，采用<code>np.zeros(shape)</code></li><li>神经网络不同层的单元数储存在变量<code>layer_dims</code>中，例如，如果<code>layer_dims</code>的值为<code>[2,4,1]</code>，则神经网络输入神经元有2个，隐层有4个神经元，输出层有1个神经元，也意味着<code>W1</code>的维度为<code>(4,2)</code>,<code>b1</code>为<code>(4,1)</code>，<code>W2</code>为<code>(1,4)</code>,<code>b2</code>为<code>(1,1)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_deep</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></div><div class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></div><div class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">3</span>)</div><div class="line">    parameters = &#123;&#125;</div><div class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="number">1</span>]) * <span class="number">0.01</span></div><div class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class="number">1</span>]))</div><div class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</div><div class="line"></div><div class="line">        </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure><p><strong>Expected output:</strong></p><table><thead><tr><th>name</th><th>value</th></tr></thead><tbody><tr><td>W1</td><td>[[ 0.01788628 0.0043651 0.00096497 -0.01863493 -0.00277388] [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218] [-0.01313865 0.00884622 0.00881318 0.01709573 0.00050034] [-0.00404677 -0.0054536 -0.01546477 0.00982367 -0.01101068]]</td></tr><tr><td>b1</td><td>[[ 0.] [ 0.] [ 0.] [ 0.]]</td></tr><tr><td>W2</td><td>[[-0.01185047 -0.0020565 0.01486148 0.00236716] [-0.01023785 -0.00712993 0.00625245 -0.00160513] [-0.00768836 -0.00230031 0.00745056 0.01976111]]</td></tr><tr><td>b2</td><td>[[ 0.] [ 0.] [ 0.]]</td></tr></tbody></table><h2 id="Forward-propagation-module"><a href="#Forward-propagation-module" class="headerlink" title="Forward propagation module"></a>Forward propagation module</h2><h3 id="Linear-Forward"><a href="#Linear-Forward" class="headerlink" title="Linear Forward"></a>Linear Forward</h3><p>本模块按顺序实现了一下函数：</p><ul><li>LINEAR</li><li>LINEAR-&gt;ACTIVATION，其中激活函数是ReLU或者Sigmoid</li><li>[LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</li></ul><p>线性前向模块主要实现如下公式：<br>$$Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}$$<br>其中$A^{[0]}=X$</p><p><strong>Excersice</strong>： 实现前向传播的线性部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></div><div class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></div><div class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></div><div class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></div><div class="line">    Z = np.dot(W, A) + b</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</div><div class="line">    cache = (A, W, b)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Z, cache</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">A, W, b = linear_forward_test_case()</div><div class="line"></div><div class="line">Z, linear_cache = linear_forward(A, W, b)</div><div class="line">print(<span class="string">"Z = "</span> + str(Z))</div></pre></td></tr></table></figure><h3 id="Linear-Activation-Forward"><a href="#Linear-Activation-Forward" class="headerlink" title="Linear-Activation Forward"></a>Linear-Activation Forward</h3><p>本文将会用到两种激活函数：</p><ul><li><strong>Sigmoid</strong>: $\sigma(Z)=\sigma(WA+b)=\frac{1}{1+e^{-(WA+b)}}$，这个函数返回两项值：激活后的值’a’和包含’Z’的’cache’。调用方法为：<br><code>A, activation_cache = sigmoid(Z)</code></li><li><strong>ReLU</strong>:ReLU函数的数学形式是 $A=RELU(Z)=\max(0,Z)$，这个函数返回两项值：激活后的值’a’和包含’Z’的’cache’。调用方法为：<br><code>A, activation_cache = relu(Z)</code></li></ul><p><strong>Excersice</strong>: 实现前向传播中的LINEAR-&gt;ACTIVATION层，即$A^{[l]}=g(Z^{[l]})=g(W^{[l]}A^{[l-1]}+b^{[l]})$，其中’g’可以是<code>sigmoid()</code>或者<code>relu()</code>。用<code>linear_forward()</code>和对应的激活函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: linear_activation_forward</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></div><div class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></div><div class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></div><div class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></div><div class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></div><div class="line"><span class="string">             stored for computing the backward pass efficiently</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</div><div class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</div><div class="line">        A, activation_cache = sigmoid(Z)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</div><div class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</div><div class="line">        A, activation_cache = relu(Z)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</div><div class="line">    cache = (linear_cache, activation_cache)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> A, cache</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">A_prev, W, b = linear_activation_forward_test_case()</div><div class="line"></div><div class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>)</div><div class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A))</div><div class="line"></div><div class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>)</div><div class="line">print(<span class="string">"With ReLU: A = "</span> + str(A))</div></pre></td></tr></table></figure><h3 id="L-layer-model"><a href="#L-layer-model" class="headerlink" title="L-layer model"></a>L-layer model</h3><p>对于有L层的神经网络来说，前向传播由L-1个<code>linear_activation_forward</code>with RELU和一个<code>linear_activation_forward</code>with SIGMOID构成。</p><p><img src="/2018/02/01/build-deep-neural-network/model_architecture.png" alt=""> </p><p><strong>Excersice</strong>：实现上述模型的前向传播</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"># GRADED FUNCTION: L_model_forward</div><div class="line"></div><div class="line">def L_model_forward(X, parameters):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    X -- data, numpy array of shape (input size, number of examples)</div><div class="line">    parameters -- output of initialize_parameters_deep()</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    AL -- last post-activation value</div><div class="line">    caches -- list of caches containing:</div><div class="line">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</div><div class="line">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</div><div class="line">    &quot;&quot;&quot;</div><div class="line"></div><div class="line">    caches = []</div><div class="line">    A = X</div><div class="line">    L = len(parameters) // 2                  # number of layers in the neural network</div><div class="line">    </div><div class="line">    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</div><div class="line">    for l in range(1, L):</div><div class="line">        A_prev = A </div><div class="line">        ### START CODE HERE ### (≈ 2 lines of code)</div><div class="line">        A, cache = linear_activation_forward(A_prev, </div><div class="line">                                             parameters[&apos;W&apos; + str(l)], </div><div class="line">                                             parameters[&apos;b&apos; + str(l)], </div><div class="line">                                             activation=&apos;relu&apos;)</div><div class="line">        caches.append(cache)</div><div class="line">        </div><div class="line">        ### END CODE HERE ###</div><div class="line">    </div><div class="line">    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</div><div class="line">    ### START CODE HERE ### (≈ 2 lines of code)</div><div class="line">    AL, cache = linear_activation_forward(A, </div><div class="line">                                          parameters[&apos;W&apos; + str(L)], </div><div class="line">                                          parameters[&apos;b&apos; + str(L)], </div><div class="line">                                          activation=&apos;sigmoid&apos;)</div><div class="line">    caches.append(cache)</div><div class="line">    </div><div class="line">    ### END CODE HERE ###</div><div class="line">    </div><div class="line">    assert(AL.shape == (1, X.shape[1]))</div><div class="line">            </div><div class="line">    return AL, caches</div><div class="line">`</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X, parameters = L_model_forward_test_case()</div><div class="line">AL, caches = L_model_forward(X, parameters)</div><div class="line">print(<span class="string">"AL = "</span> + str(AL))</div><div class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)))</div></pre></td></tr></table></figure><h2 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h2><p><strong>Excersice</strong>：计算交叉熵损失（cross-entropy cost）$J$，公式如下：<br>$$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right)) \tag{7}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the cost function defined by equation (7).</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></div><div class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    cost -- cross-entropy cost</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    m = Y.shape[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># Compute loss from aL and y.</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></div><div class="line">    cost = (<span class="number">-1</span> / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL)))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></div><div class="line">    <span class="keyword">assert</span>(cost.shape == ())</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> cost</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Y, AL = compute_cost_test_case()</div><div class="line"></div><div class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)))</div></pre></td></tr></table></figure><h2 id="Backward-propagation-module"><a href="#Backward-propagation-module" class="headerlink" title="Backward propagation module"></a>Backward propagation module</h2><p><strong>Reminder</strong>:<br><img src="/2018/02/01/build-deep-neural-network/backprop.png" alt=""></p><p>与前向传播类似，后向传播的实现也分成三个步骤：</p><ul><li>LINEAR backward</li><li>LINEAR-&gt;ACTIVATION backward，其中ACTIVATION是ReLU或者sigmoid函数</li><li>[LINEAR-&gt;RELU]× (L-1)-&gt;LINEAR-&gt;SIGMOID backward</li></ul><h3 id="Linear-backward"><a href="#Linear-backward" class="headerlink" title="Linear backward"></a>Linear backward</h3><p>对于第l层神经网络，线性部分是：$Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}$。假设已知$d Z^{[l]} = \frac{\partial L}{\partial Z^{[l]}}$，需要求的是 $dW^{[l]}$, $db^{[l]}$以及$dA^{[l-1]}$。</p><p><img src="/2018/02/01/build-deep-neural-network/linearback.png" width="30%"></p><p>三个输出$dW^{[l]}$, $db^{[l]}$以及$dA^{[l-1]}$可以用输入$d Z^{[l]}$ 来计算:<br>$$dW^{[l]}=\frac{\partial L}{\partial W^{[l]}} = \frac{1}{m}d Z^{[l]}A^{[l-1]T}$$<br>$$db^{[l]}=\frac{\partial L}{\partial b^{[l]}} = \frac{1}{m} \sum^m_{i=1}dZ^{<a href="i">l</a>}$$<br>$$dA^{[l-1]} = \frac{\partial L}{\partial A^{[l-1]}} = W^{[l]T}dZ^{[l]}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></div><div class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></div><div class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></div><div class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></div><div class="line"><span class="string">    """</span></div><div class="line">    A_prev, W, b = cache</div><div class="line">    m = A_prev.shape[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">    dW = np.dot(dZ, cache[<span class="number">0</span>].T) / m</div><div class="line">    db = np.squeeze(np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)) / m</div><div class="line">    dA_prev = np.dot(cache[<span class="number">1</span>].T, dZ)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</div><div class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</div><div class="line">    <span class="keyword">assert</span> (isinstance(db, float))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dA_prev, dW, db</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Set up some test inputs</span></div><div class="line">dZ, linear_cache = linear_backward_test_case()</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</div></pre></td></tr></table></figure><h3 id="Linear-Activation-backward"><a href="#Linear-Activation-backward" class="headerlink" title="Linear-Activation backward"></a>Linear-Activation backward</h3><ul><li><p><strong>sigmoid_backward</strong> 实现了SIGMOID函数的后向传播，调用方法:<br><code>dZ = sigmoid_backward(dA, activation_cache)</code></p></li><li><p><strong>relu_backward</strong> 实现了relu函数的后向传播，调用方法：<br><code>dZ = relu_backward(dA, activation_cache)</code></p></li></ul><p>如果$g(.)$是激活函数，sigmoid_backward和relu_backward的计算是：<br>$$dZ^{[l]} = dA^{[l]}*g’(Z^{[l]})$$</p><p><strong>Excersice</strong>:实现LINEAR-&gt;ACTIVATION层的后向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></div><div class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></div><div class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></div><div class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></div><div class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></div><div class="line"><span class="string">    """</span></div><div class="line">    linear_cache, activation_cache = cache</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        dZ = relu_backward(dA, activation_cache)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        dZ = sigmoid_backward(dA, activation_cache)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Shorten the code</span></div><div class="line">    dA_prev, dW, db = linear_backward(dZ, linear_cache)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dA_prev, dW, db</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">AL, linear_activation_cache = linear_activation_backward_test_case()</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"sigmoid"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>)</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"relu"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</div></pre></td></tr></table></figure><h3 id="L-Model-backward"><a href="#L-Model-backward" class="headerlink" title="L-Model backward"></a>L-Model backward</h3><p>当实现L_model_forward函数时，在每次迭代中都保存了(X,W,b,Z)的值，在后向传播模块中，你将用到这些值来计算梯度。</p><p><img src="/2018/02/01/build-deep-neural-network/mn_backward.png" width="100%"></p><p><strong>Initializing backpropagation</strong>:  上述网络的输出是：$A^{[L]} = \sigma(Z^{[L]})$。因此需要计算$dAL = \frac{\partial L}{\partial A^{[L]}}$.</p><p><code>dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></p><p><strong>Excercise</strong>:实现后向传播:[LINEAR-&gt;RELU] × (l-1) -&gt; LINEAR -&gt; SIGMOID 模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></div><div class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></div><div class="line"><span class="string">    caches -- list of caches containing:</span></div><div class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></div><div class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    grads -- A dictionary with the gradients</span></div><div class="line"><span class="string">             grads["dA" + str(l)] = ... </span></div><div class="line"><span class="string">             grads["dW" + str(l)] = ...</span></div><div class="line"><span class="string">             grads["db" + str(l)] = ... </span></div><div class="line"><span class="string">    """</span></div><div class="line">    grads = &#123;&#125;</div><div class="line">    L = len(caches) <span class="comment"># the number of layers</span></div><div class="line">    m = AL.shape[<span class="number">1</span>]</div><div class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></div><div class="line">    </div><div class="line">    <span class="comment"># Initializing the backpropagation</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></div><div class="line">    dAL = dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></div><div class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></div><div class="line">    current_cache = caches[<span class="number">-1</span>]</div><div class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_backward(sigmoid_backward(dAL, </div><div class="line">                                                                                                        current_cache[<span class="number">1</span>]), </div><div class="line">                                                                                       current_cache[<span class="number">0</span>])</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</div><div class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></div><div class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></div><div class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></div><div class="line">        current_cache = caches[l]</div><div class="line">        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, caches[<span class="number">1</span>]), caches[<span class="number">0</span>])</div><div class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</div><div class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</div><div class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> grads</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X_assess, Y_assess, AL, caches = L_model_backward_test_case()</div><div class="line">grads = L_model_backward(AL, Y_assess, caches)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA1 = "</span>+ str(grads[<span class="string">"dA1"</span>]))</div></pre></td></tr></table></figure><h2 id="Update-parameters"><a href="#Update-parameters" class="headerlink" title="Update parameters"></a>Update parameters</h2><p>在这一节，你将用梯度下降法更新模型参数：<br>$$b^{[l]} = b^{[l]} - \alpha db^{[l]}$$<br>其中$\alpha$是学习率。</p><p><strong>Excercise</strong>:实现<code>update_parameters()</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Update parameters using gradient descent</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></div><div class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></div><div class="line"></div><div class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]</div><div class="line">        parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">parameters, grads = update_parameters_test_case()</div><div class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"W3 = "</span> + str(parameters[<span class="string">"W3"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b3 = "</span> + str(parameters[<span class="string">"b3"</span>]))</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;源链接：&lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.coursera.org/learn
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 + Anaconda 中安装 Tensorflow + MuJoCo</title>
    <link href="https://zhuoyangdu.github.io/2018/01/20/install-mujoco/"/>
    <id>https://zhuoyangdu.github.io/2018/01/20/install-mujoco/</id>
    <published>2018-01-19T16:03:10.000Z</published>
    <updated>2018-01-19T16:15:03.984Z</updated>
    
    <content type="html"><![CDATA[<p>为了做伯克利深度强化学习的课程作业安装了Tensorflow+MoJoCo+OpenAI gym，记录下安装过程和自己踩过的坑~</p><p><strong>安装Anaconda和tensorflow</strong></p><p>Anaconda的安装过程可直接参照官网，选择安装3.5版本的python，新建conda运行环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Python 3.5</div><div class="line">$ conda create -n tensorflow python=3.5</div></pre></td></tr></table></figure><p>选择pip工具安装Tensorflow，首先需要激活conda环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ source activate tensorflow</div></pre></td></tr></table></figure><p>根据要安装的不同版本的tensorflow设置环境变量（操作系统，Python版本，CPU版本还是CPU+GPU版本）。本文环境为：Ubuntu/Linux 64-bit, CPU-only, Python3.5。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.5  </div><div class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl</div></pre></td></tr></table></figure><p>根据Python版本进行安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Python 3  </div><div class="line">(tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure><hr><p><strong>安装MuJoCo</strong></p><p>安装教程可参见：<a href="https://github.com/openai/mujoco-py" target="_blank" rel="external">https://github.com/openai/mujoco-py</a></p><p>安装过程主要分两步，需要首先安装MuJoCo，然后安装mujoco-py，由于课程要求使用mujoco1.31，因此这里安装1.31的版本，注意mujoco-py的版本需要和MuJoCo版本对应上，如果安装mujoco1.31，则mujoco-py需要安装0.5.7的版本，否则会安装失败。</p><p>Linux版本需要首先安装依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ sudo apt-get install -y curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev python3-pip python3-numpy python3-scipy net-tools unzip vim wget xpra</div></pre></td></tr></table></figure><p>安装MuJoCo：</p><ol><li><p>在<a href="https://www.roboti.us/license.html" target="_blank" rel="external">MuJoCo Website</a>申请30天试用或学生免费证书，这里需要获取Computer id，下载对应系统(Linux)的脚本运行得到id（脚本需要设置运行权限 <code>chmod 777 getid_linux</code>），邮件中会收到license文件mjkey.txt。</p></li><li><p>下载对应版本的mjpro，<a href="https://www.roboti.us/index.html" target="_blank" rel="external">https://www.roboti.us/index.html</a>，本文选择mjpro131 linux。</p></li><li><p>解压<code>mjpro131.zip</code>到<code>~/.mujoco/mjpro131</code>，并将license文件放在<code>~/.mujoco/</code>和<code>~/.mujoco/mjpro131/bin</code>目录下。</p></li><li><p>设置环境变量：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export LD_LIBRARY_PATH=/home/user/.mujoco/mjpro131/bin</div></pre></td></tr></table></figure><p> <em>测试是否安装成功：</em></p><p> 在<code>~/.mujoco/mjpro131/bin</code>下运行<code>./simulate ../model/humanoid.xml</code>，弹出仿真界面即为安装成功。</p><p>安装mujoco-py:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ pip3 install mujoco-py==0.5.7</div></pre></td></tr></table></figure><hr><p><strong>安装OpenAI gym</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ git clone https://github.com/openai/gym.git</div><div class="line">(tensorflow)$ cd gym</div><div class="line">(tensorflow)$ pip install -e .</div></pre></td></tr></table></figure><hr><p>测试：通过运行<code>./homework/hw1/demo.bash</code>测试是否安装成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了做伯克利深度强化学习的课程作业安装了Tensorflow+MoJoCo+OpenAI gym，记录下安装过程和自己踩过的坑~&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安装Anaconda和tensorflow&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anaconda的安装过程可直接参照官
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://zhuoyangdu.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>深度强化学习资源整理</title>
    <link href="https://zhuoyangdu.github.io/2017/11/07/DRL-references-summarizing/"/>
    <id>https://zhuoyangdu.github.io/2017/11/07/DRL-references-summarizing/</id>
    <published>2017-11-07T12:29:11.000Z</published>
    <updated>2017-11-07T12:55:01.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>公开课</strong></p><ul><li><p>David Silver UCL Course 强化学习公开课：<br>  <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="external">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a><br>  读书笔记：<br>  <a href="https://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html" target="_blank" rel="external">https://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html</a></p></li><li><p>MIT: Deep Learning for Self-driving Cars:<br>  <a href="http://selfdrivingcars.mit.edu/" target="_blank" rel="external">http://selfdrivingcars.mit.edu/</a></p></li><li><p>BerkeleyX: CS188x_1: Artificial Intelligence:<br>  <a href="https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/20021a0a32d14a31b087db8d4bb582fd/" target="_blank" rel="external">https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/20021a0a32d14a31b087db8d4bb582fd/</a></p></li></ul><hr><p><strong>论文</strong></p><ul><li><p>ICML 2017 tutorial: Deep Reinforcement Learning, Decision Making, and Control<br>  <a href="https://sites.google.com/view/icml17deeprl" target="_blank" rel="external">https://sites.google.com/view/icml17deeprl</a></p></li><li><p>Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning：<br>  原文：<a href="https://arxiv.org/abs/1705.01196" target="_blank" rel="external">https://arxiv.org/abs/1705.01196</a><br>  机器之心中文翻译：<a href="https://www.jiqizhixin.com/articles/2017-07-27-3" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-07-27-3</a></p></li></ul><hr><p><strong>博客</strong></p><ul><li>Kintoki <a href="http://www.cnblogs.com/jinxulin/tag/Reinforcement%20learning/" target="_blank" rel="external">http://www.cnblogs.com/jinxulin/tag/Reinforcement%20learning/</a></li></ul><hr><p><strong>平台</strong></p><ul><li>TORCS仿真软件： <a href="http://torcs.sourceforge.net/" target="_blank" rel="external">http://torcs.sourceforge.net/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;公开课&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;David Silver UCL Course 强化学习公开课：&lt;br&gt;  &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://zhuoyangdu.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
</feed>
