<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="default">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">









  <link rel="alternate" href="/atom.xml" title="Zhuoyang Du" type="application/atom+xml" />






<meta name="description" content="You only live once.">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhuoyang Du">
<meta property="og:url" content="https://zhuoyangdu.github.io/index.html">
<meta property="og:site_name" content="Zhuoyang Du">
<meta property="og:description" content="You only live once.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhuoyang Du">
<meta name="twitter:description" content="You only live once.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"right","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhuoyangdu.github.io/"/>





  <title>Zhuoyang Du</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhuoyang Du</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhuoyangdu.github.io/2018/02/01/build-deep-neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhuoyang Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuoyang Du">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/01/build-deep-neural-network/" itemprop="url">Building deep neural network - step by step</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-01T16:43:03+08:00">
                2018-02-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>源链接：<a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="external">https://www.coursera.org/learn/neural-networks-deep-learning/</a> Week4 assignment(part 1 of 2)</p>
<ul>
<li>这周内容：实现所有建立深度神经网络需要的函数</li>
<li>下周内容：构建用于图像分类的深度神经网络</li>
</ul>
<p><strong> 本次作业的要求是</strong></p>
<ul>
<li>用非线性单元（如：ReLU）来改进模型</li>
<li>建立深度神经网络</li>
<li>实现易于实用的神经网络</li>
</ul>
<p><strong>符号标记</strong></p>
<ul>
<li>上标 $[l]$ 表示与第$l$层相关的量<ul>
<li>例：$a^{[L]}$是第$L$层激活层，$W^{[L]}$和$b^{[L]}$分别是第$L$层的参数。</li>
</ul>
</li>
<li>上标$(i)$表示与第$i$个样本相关的量<ul>
<li>例：$x^(i)$是第$i$个训练样本</li>
</ul>
</li>
<li>下标$i$表示某个向量的第$i$个量<ul>
<li>例：$a^{[l]}_i$表示第$l$个激活层的第$i$个元素</li>
</ul>
</li>
</ul>
<h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>首先导入你需要用到的所有python库。</p>
<ul>
<li><code>numpy</code></li>
<li><code>matplotlib</code></li>
<li><code>dnn_utils</code>：提供了本文需要用到的一些必要的函数</li>
<li><code>testCases</code>：提供了一些测试实例</li>
<li><code>np.random.seed(1)</code>用来保证所有的随机函数都是一致的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> h5py</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></div><div class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></div><div class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></div><div class="line"></div><div class="line">%load_ext autoreload</div><div class="line">%autoreload <span class="number">2</span></div><div class="line"></div><div class="line">np.random.seed(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<h2 id="Outline-of-the-Assignment"><a href="#Outline-of-the-Assignment" class="headerlink" title="Outline of the Assignment"></a>Outline of the Assignment</h2><ul>
<li>初始化两层神经网络和$L$层神经网络的参数</li>
<li>实现前向传播模块（forward propagation module）<ul>
<li>实现某一层前向传播的线性部分（LINEAR）（结果为$Z^{[l]}$）</li>
<li>给定激活函数ACTIVATION（relu/sigmoid）</li>
<li>结合前两步实现新的[LINEAR-&gt;ACTIVATION]前向函数</li>
<li>堆叠[LINEAR-&gt;ACTIVATION]前向函数$L-1$次（从第1层到第$L-1$层，在结尾加上[LINEAR-&gt;SIGMOID]作为最后第$L$层），则得到新的函数 L_model_forward。</li>
</ul>
</li>
<li>计算损失</li>
<li>实现后向传播模块<ul>
<li>计算某一层后向传播的线性部分LINEAR。</li>
<li>给定激活函数ACTIVATION的梯度（relu_backward/sigmoid_backward）。</li>
<li>结合前面两步得到新的后向函数[LINEAR-&gt;ACTIVATION]。</li>
<li>堆叠[LINEAR-&gt;ACTIVATION]后向函数$L-1$次，并加入[LINEAR-&gt;SIGMOID]后向函数，得到新的函数 L_model_backward。</li>
</ul>
</li>
<li>最后更新参数。</li>
</ul>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>下文实现了两个初始化的函数，第一个是用来初始化两层神经网络，第二个是$L$层神经网络。</p>
<h3 id="2-lay-Neural-Network"><a href="#2-lay-Neural-Network" class="headerlink" title="2-lay Neural Network"></a>2-lay Neural Network</h3><p><strong>Exercise</strong>: 创建并初始化二层神经网络</p>
<p><strong>Instructions</strong>: </p>
<ul>
<li>模型结构是LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</li>
<li>权重矩阵随机初始化，用：<code>np.random.randn(shape)*0.01</code></li>
<li>偏差初始化为0: <code>mp.zeros(shape)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Argument:</span></div><div class="line"><span class="string">    n_x -- size of the input layer</span></div><div class="line"><span class="string">    n_h -- size of the hidden layer</span></div><div class="line"><span class="string">    n_y -- size of the output layer</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></div><div class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></div><div class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></div><div class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></div><div class="line">    b1 = np.zeros(shape=(n_h, <span class="number">1</span>))</div><div class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></div><div class="line">    b2 = np.zeros(shape=(n_y, <span class="number">1</span>))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</div><div class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</div><div class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</div><div class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</div><div class="line">    </div><div class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</div><div class="line">                  <span class="string">"b1"</span>: b1,</div><div class="line">                  <span class="string">"W2"</span>: W2,</div><div class="line">                  <span class="string">"b2"</span>: b2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">parameters = initialize_parameters(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>W1</strong></td>
<td>[[ 0.01624345 -0.00611756] [-0.00528172 -0.01072969]]</td>
</tr>
<tr>
<td><strong>W2</strong></td>
<td>[[ 0.00865408 -0.02301539]]</td>
</tr>
<tr>
<td><strong>b1</strong></td>
<td>[[ 0.] [ 0.]]</td>
</tr>
<tr>
<td><strong>b2</strong></td>
<td>[[ 0.]]</td>
</tr>
</tbody>
</table>
<h3 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h3><p>L层神经网络的初始化要相对复杂一些。初始化的时候要注意匹配每一层的维度，$n^{[l]}$表示第$l$层的神经元数量，如果输入$X$是$(12288,209)$（有209个训练样本），则：</p>
<table>
<thead>
<tr>
<th></th>
<th>shape of W</th>
<th>shape of b</th>
<th>Activation</th>
<th>Shape of Activation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Layer1</td>
<td>$(n^{[1]},12288)$</td>
<td>$(n^{[1]},1)$</td>
<td>$Z^{[1]}=W^{[1]}X+b^{[1]}$</td>
<td>$(n^{[1]},209)$</td>
</tr>
<tr>
<td>Layer2</td>
<td>$(n^{[2]},n^{[1]})$</td>
<td>$(n^{[2]},1)$</td>
<td>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</td>
<td>$(n^{[2]},209)$</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>Layer L-1</td>
<td>$(n^{[L-1]},n^{[L-2]})$</td>
<td>$(n^{[L-1]},1)$</td>
<td>$Z^{[L-1]}=W^{[L-1]}A^{[L-2]}+b^{[L-1]}$</td>
<td>$(n^{[L-1]},209)$</td>
</tr>
<tr>
<td>Layer L</td>
<td>$(n^{[L]},n^{[L-1]})$</td>
<td>$(n^{[L]},1)$</td>
<td>$Z^{[L]}=W^{[L]}A^{[L-1]}+b^{[L]}$</td>
<td>$(n^{[L]},209)$</td>
</tr>
</tbody>
</table>
<p>需要注意的是，由于python的<strong>broadcast</strong>特性，如果计算 <code>WX+b</code>，最后得到的结果是：<br>$$W = \begin{bmatrix} j &amp; k &amp; l \\ m &amp; n &amp; o \\ p &amp; q &amp; r \end{bmatrix},<br> X =  \begin{bmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{bmatrix},<br> b = \begin{bmatrix} s \\ t \\ u \end{bmatrix}$$<br>$$WX+b = \begin{bmatrix} (ja+kd+lg)+s &amp; (jb+ke+lh)+s &amp; (jc+kf+li)+s \\<br>(ma+nd+og)+t &amp; (mb+ne+oh)+t &amp; (mc+nf+oi)+t \\<br>(pa+qd+rg)+u &amp; (pb+qe+rh)+u &amp; (pc+qf+ri)+u\end{bmatrix}$$</p>
<p><strong>Excercise</strong> 实现L层神经网络的初始化</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>模型结构是[LINEAR-&gt;RELU] × (L-1)-&gt;LINEAR-&gt;SIGMOID. 由L-1层RELU激活函数和带有sigmoid函数的输出层构成。</li>
<li>权重矩阵随机初始化，采用<code>np.random.rand(shape)*0.01</code></li>
<li>偏差向量零初始化，采用<code>np.zeros(shape)</code></li>
<li>神经网络不同层的单元数储存在变量<code>layer_dims</code>中，例如，如果<code>layer_dims</code>的值为<code>[2,4,1]</code>，则神经网络输入神经元有2个，隐层有4个神经元，输出层有1个神经元，也意味着<code>W1</code>的维度为<code>(4,2)</code>,<code>b1</code>为<code>(4,1)</code>，<code>W2</code>为<code>(1,4)</code>,<code>b2</code>为<code>(1,1)</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_deep</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></div><div class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></div><div class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">3</span>)</div><div class="line">    parameters = &#123;&#125;</div><div class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="number">1</span>]) * <span class="number">0.01</span></div><div class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class="number">1</span>]))</div><div class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</div><div class="line"></div><div class="line">        </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure>
<p><strong>Expected output:</strong></p>
<table>
<thead>
<tr>
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>W1</td>
<td>[[ 0.01788628 0.0043651 0.00096497 -0.01863493 -0.00277388] [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218] [-0.01313865 0.00884622 0.00881318 0.01709573 0.00050034] [-0.00404677 -0.0054536 -0.01546477 0.00982367 -0.01101068]]</td>
</tr>
<tr>
<td>b1</td>
<td>[[ 0.] [ 0.] [ 0.] [ 0.]]</td>
</tr>
<tr>
<td>W2</td>
<td>[[-0.01185047 -0.0020565 0.01486148 0.00236716] [-0.01023785 -0.00712993 0.00625245 -0.00160513] [-0.00768836 -0.00230031 0.00745056 0.01976111]]</td>
</tr>
<tr>
<td>b2</td>
<td>[[ 0.] [ 0.] [ 0.]]</td>
</tr>
</tbody>
</table>
<h2 id="Forward-propagation-module"><a href="#Forward-propagation-module" class="headerlink" title="Forward propagation module"></a>Forward propagation module</h2><h3 id="Linear-Forward"><a href="#Linear-Forward" class="headerlink" title="Linear Forward"></a>Linear Forward</h3><p>本模块按顺序实现了一下函数：</p>
<ul>
<li>LINEAR</li>
<li>LINEAR-&gt;ACTIVATION，其中激活函数是ReLU或者Sigmoid</li>
<li>[LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</li>
</ul>
<p>线性前向模块主要实现如下公式：<br>$$Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}$$<br>其中$A^{[0]}=X$</p>
<p><strong>Excersice</strong>： 实现前向传播的线性部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></div><div class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></div><div class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></div><div class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></div><div class="line">    Z = np.dot(W, A) + b</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</div><div class="line">    cache = (A, W, b)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Z, cache</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">A, W, b = linear_forward_test_case()</div><div class="line"></div><div class="line">Z, linear_cache = linear_forward(A, W, b)</div><div class="line">print(<span class="string">"Z = "</span> + str(Z))</div></pre></td></tr></table></figure>
<h3 id="Linear-Activation-Forward"><a href="#Linear-Activation-Forward" class="headerlink" title="Linear-Activation Forward"></a>Linear-Activation Forward</h3><p>本文将会用到两种激活函数：</p>
<ul>
<li><strong>Sigmoid</strong>: $\sigma(Z)=\sigma(WA+b)=\frac{1}{1+e^{-(WA+b)}}$，这个函数返回两项值：激活后的值’a’和包含’Z’的’cache’。调用方法为：<br><code>A, activation_cache = sigmoid(Z)</code></li>
<li><strong>ReLU</strong>:ReLU函数的数学形式是 $A=RELU(Z)=\max(0,Z)$，这个函数返回两项值：激活后的值’a’和包含’Z’的’cache’。调用方法为：<br><code>A, activation_cache = relu(Z)</code></li>
</ul>
<p><strong>Excersice</strong>: 实现前向传播中的LINEAR-&gt;ACTIVATION层，即$A^{[l]}=g(Z^{[l]})=g(W^{[l]}A^{[l-1]}+b^{[l]})$，其中’g’可以是<code>sigmoid()</code>或者<code>relu()</code>。用<code>linear_forward()</code>和对应的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: linear_activation_forward</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></div><div class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></div><div class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></div><div class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></div><div class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></div><div class="line"><span class="string">             stored for computing the backward pass efficiently</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</div><div class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</div><div class="line">        A, activation_cache = sigmoid(Z)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</div><div class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</div><div class="line">        A, activation_cache = relu(Z)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</div><div class="line">    cache = (linear_cache, activation_cache)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> A, cache</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">A_prev, W, b = linear_activation_forward_test_case()</div><div class="line"></div><div class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>)</div><div class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A))</div><div class="line"></div><div class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>)</div><div class="line">print(<span class="string">"With ReLU: A = "</span> + str(A))</div></pre></td></tr></table></figure>
<h3 id="L-layer-model"><a href="#L-layer-model" class="headerlink" title="L-layer model"></a>L-layer model</h3><p>对于有L层的神经网络来说，前向传播由L-1个<code>linear_activation_forward</code>with RELU和一个<code>linear_activation_forward</code>with SIGMOID构成。</p>
<p><img src="/2018/02/01/build-deep-neural-network/model_architecture.png" alt=""> </p>
<p><strong>Excersice</strong>：实现上述模型的前向传播</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"># GRADED FUNCTION: L_model_forward</div><div class="line"></div><div class="line">def L_model_forward(X, parameters):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    X -- data, numpy array of shape (input size, number of examples)</div><div class="line">    parameters -- output of initialize_parameters_deep()</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    AL -- last post-activation value</div><div class="line">    caches -- list of caches containing:</div><div class="line">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</div><div class="line">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</div><div class="line">    &quot;&quot;&quot;</div><div class="line"></div><div class="line">    caches = []</div><div class="line">    A = X</div><div class="line">    L = len(parameters) // 2                  # number of layers in the neural network</div><div class="line">    </div><div class="line">    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</div><div class="line">    for l in range(1, L):</div><div class="line">        A_prev = A </div><div class="line">        ### START CODE HERE ### (≈ 2 lines of code)</div><div class="line">        A, cache = linear_activation_forward(A_prev, </div><div class="line">                                             parameters[&apos;W&apos; + str(l)], </div><div class="line">                                             parameters[&apos;b&apos; + str(l)], </div><div class="line">                                             activation=&apos;relu&apos;)</div><div class="line">        caches.append(cache)</div><div class="line">        </div><div class="line">        ### END CODE HERE ###</div><div class="line">    </div><div class="line">    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</div><div class="line">    ### START CODE HERE ### (≈ 2 lines of code)</div><div class="line">    AL, cache = linear_activation_forward(A, </div><div class="line">                                          parameters[&apos;W&apos; + str(L)], </div><div class="line">                                          parameters[&apos;b&apos; + str(L)], </div><div class="line">                                          activation=&apos;sigmoid&apos;)</div><div class="line">    caches.append(cache)</div><div class="line">    </div><div class="line">    ### END CODE HERE ###</div><div class="line">    </div><div class="line">    assert(AL.shape == (1, X.shape[1]))</div><div class="line">            </div><div class="line">    return AL, caches</div><div class="line">`</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X, parameters = L_model_forward_test_case()</div><div class="line">AL, caches = L_model_forward(X, parameters)</div><div class="line">print(<span class="string">"AL = "</span> + str(AL))</div><div class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)))</div></pre></td></tr></table></figure>
<h2 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h2><p><strong>Excersice</strong>：计算交叉熵损失（cross-entropy cost）$J$，公式如下：<br>$$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L] (i)}\right))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the cost function defined by equation (7).</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></div><div class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    cost -- cross-entropy cost</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    m = Y.shape[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># Compute loss from aL and y.</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></div><div class="line">    cost = (<span class="number">-1</span> / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL)))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></div><div class="line">    <span class="keyword">assert</span>(cost.shape == ())</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> cost</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Y, AL = compute_cost_test_case()</div><div class="line"></div><div class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)))</div></pre></td></tr></table></figure>
<h2 id="Backward-propagation-module"><a href="#Backward-propagation-module" class="headerlink" title="Backward propagation module"></a>Backward propagation module</h2><p><strong>Reminder</strong>:<br><img src="/2018/02/01/build-deep-neural-network/backprop.png" alt=""></p>
<p>与前向传播类似，后向传播的实现也分成三个步骤：</p>
<ul>
<li>LINEAR backward</li>
<li>LINEAR-&gt;ACTIVATION backward，其中ACTIVATION是ReLU或者sigmoid函数</li>
<li>[LINEAR-&gt;RELU]× (L-1)-&gt;LINEAR-&gt;SIGMOID backward</li>
</ul>
<h3 id="Linear-backward"><a href="#Linear-backward" class="headerlink" title="Linear backward"></a>Linear backward</h3><p>对于第l层神经网络，线性部分是：$Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}$。假设已知$d Z^{[l]} = \frac{\partial L}{\partial Z^{[l]}}$，需要求的是 $dW^{[l]}$, $db^{[l]}$以及$dA^{[l-1]}$。</p>
<p><img src="/2018/02/01/build-deep-neural-network/linearback.png" width="30%"></p>
<p>三个输出$dW^{[l]}$, $db^{[l]}$以及$dA^{[l-1]}$可以用输入$d Z^{[l]}$ 来计算:</p>
<p>$$dW^{[l]}=\frac{\partial L}{\partial W^{[l]}} = \frac{1}{m}d Z^{[l]}A^{[l-1]T}$$</p>
<p>$$ db^{[l]}=\frac{\partial L}{\partial b^{[l]}} = \frac{1}{m} \sum^m_{i=1}dZ^{[l] (i)} $$</p>
<p>$$dA^{[l-1]} = \frac{\partial L}{\partial A^{[l-1]}} = W^{[l]T}dZ^{[l]}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></div><div class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></div><div class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></div><div class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></div><div class="line"><span class="string">    """</span></div><div class="line">    A_prev, W, b = cache</div><div class="line">    m = A_prev.shape[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">    dW = np.dot(dZ, cache[<span class="number">0</span>].T) / m</div><div class="line">    db = np.squeeze(np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)) / m</div><div class="line">    dA_prev = np.dot(cache[<span class="number">1</span>].T, dZ)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</div><div class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</div><div class="line">    <span class="keyword">assert</span> (isinstance(db, float))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dA_prev, dW, db</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Set up some test inputs</span></div><div class="line">dZ, linear_cache = linear_backward_test_case()</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</div></pre></td></tr></table></figure>
<h3 id="Linear-Activation-backward"><a href="#Linear-Activation-backward" class="headerlink" title="Linear-Activation backward"></a>Linear-Activation backward</h3><ul>
<li><p><strong>sigmoid_backward</strong> 实现了SIGMOID函数的后向传播，调用方法:<br><code>dZ = sigmoid_backward(dA, activation_cache)</code></p>
</li>
<li><p><strong>relu_backward</strong> 实现了relu函数的后向传播，调用方法：<br><code>dZ = relu_backward(dA, activation_cache)</code></p>
</li>
</ul>
<p>如果$g(.)$是激活函数，sigmoid_backward和relu_backward的计算是：<br>$$dZ^{[l]} = dA^{[l]}*g’(Z^{[l]})$$</p>
<p><strong>Excersice</strong>:实现LINEAR-&gt;ACTIVATION层的后向传播：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></div><div class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></div><div class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></div><div class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></div><div class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></div><div class="line"><span class="string">    """</span></div><div class="line">    linear_cache, activation_cache = cache</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        dZ = relu_backward(dA, activation_cache)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        dZ = sigmoid_backward(dA, activation_cache)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Shorten the code</span></div><div class="line">    dA_prev, dW, db = linear_backward(dZ, linear_cache)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dA_prev, dW, db</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">AL, linear_activation_cache = linear_activation_backward_test_case()</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"sigmoid"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>)</div><div class="line"></div><div class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"relu"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</div></pre></td></tr></table></figure>
<h3 id="L-Model-backward"><a href="#L-Model-backward" class="headerlink" title="L-Model backward"></a>L-Model backward</h3><p>当实现L_model_forward函数时，在每次迭代中都保存了(X,W,b,Z)的值，在后向传播模块中，你将用到这些值来计算梯度。</p>
<p><img src="/2018/02/01/build-deep-neural-network/mn_backward.png" width="100%"></p>
<p><strong>Initializing backpropagation</strong>:  上述网络的输出是：$A^{[L]} = \sigma(Z^{[L]})$。因此需要计算$dAL = \frac{\partial L}{\partial A^{[L]}}$.</p>
<p><code>dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></p>
<p><strong>Excercise</strong>:实现后向传播:[LINEAR-&gt;RELU] × (l-1) -&gt; LINEAR -&gt; SIGMOID 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></div><div class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></div><div class="line"><span class="string">    caches -- list of caches containing:</span></div><div class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></div><div class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    grads -- A dictionary with the gradients</span></div><div class="line"><span class="string">             grads["dA" + str(l)] = ... </span></div><div class="line"><span class="string">             grads["dW" + str(l)] = ...</span></div><div class="line"><span class="string">             grads["db" + str(l)] = ... </span></div><div class="line"><span class="string">    """</span></div><div class="line">    grads = &#123;&#125;</div><div class="line">    L = len(caches) <span class="comment"># the number of layers</span></div><div class="line">    m = AL.shape[<span class="number">1</span>]</div><div class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></div><div class="line">    </div><div class="line">    <span class="comment"># Initializing the backpropagation</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></div><div class="line">    dAL = dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></div><div class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></div><div class="line">    current_cache = caches[<span class="number">-1</span>]</div><div class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_backward(sigmoid_backward(dAL, </div><div class="line">                                                                                                        current_cache[<span class="number">1</span>]), </div><div class="line">                                                                                       current_cache[<span class="number">0</span>])</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</div><div class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></div><div class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></div><div class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></div><div class="line">        current_cache = caches[l]</div><div class="line">        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, caches[<span class="number">1</span>]), caches[<span class="number">0</span>])</div><div class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</div><div class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</div><div class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> grads</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X_assess, Y_assess, AL, caches = L_model_backward_test_case()</div><div class="line">grads = L_model_backward(AL, Y_assess, caches)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dA1 = "</span>+ str(grads[<span class="string">"dA1"</span>]))</div></pre></td></tr></table></figure>
<h2 id="Update-parameters"><a href="#Update-parameters" class="headerlink" title="Update parameters"></a>Update parameters</h2><p>在这一节，你将用梯度下降法更新模型参数：<br>$$b^{[l]} = b^{[l]} - \alpha db^{[l]}$$<br>其中$\alpha$是学习率。</p>
<p><strong>Excercise</strong>:实现<code>update_parameters()</code>函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Update parameters using gradient descent</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></div><div class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></div><div class="line"></div><div class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</div><div class="line">        parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]</div><div class="line">        parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">parameters, grads = update_parameters_test_case()</div><div class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"W3 = "</span> + str(parameters[<span class="string">"W3"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b3 = "</span> + str(parameters[<span class="string">"b3"</span>]))</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhuoyangdu.github.io/2018/01/20/install-mujoco/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhuoyang Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuoyang Du">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/20/install-mujoco/" itemprop="url">Ubuntu16.04 + Anaconda 中安装 Tensorflow + MuJoCo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-20T00:03:10+08:00">
                2018-01-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>为了做伯克利深度强化学习的课程作业安装了Tensorflow+MoJoCo+OpenAI gym，记录下安装过程和自己踩过的坑~</p>
<p><strong>安装Anaconda和tensorflow</strong></p>
<p>Anaconda的安装过程可直接参照官网，选择安装3.5版本的python，新建conda运行环境。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Python 3.5</div><div class="line">$ conda create -n tensorflow python=3.5</div></pre></td></tr></table></figure>
<p>选择pip工具安装Tensorflow，首先需要激活conda环境：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ source activate tensorflow</div></pre></td></tr></table></figure>
<p>根据要安装的不同版本的tensorflow设置环境变量（操作系统，Python版本，CPU版本还是CPU+GPU版本）。本文环境为：Ubuntu/Linux 64-bit, CPU-only, Python3.5。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.5  </div><div class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl</div></pre></td></tr></table></figure>
<p>根据Python版本进行安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Python 3  </div><div class="line">(tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure>
<hr>
<p><strong>安装MuJoCo</strong></p>
<p>安装教程可参见：<a href="https://github.com/openai/mujoco-py" target="_blank" rel="external">https://github.com/openai/mujoco-py</a></p>
<p>安装过程主要分两步，需要首先安装MuJoCo，然后安装mujoco-py，由于课程要求使用mujoco1.31，因此这里安装1.31的版本，注意mujoco-py的版本需要和MuJoCo版本对应上，如果安装mujoco1.31，则mujoco-py需要安装0.5.7的版本，否则会安装失败。</p>
<p>Linux版本需要首先安装依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ sudo apt-get install -y curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev python3-pip python3-numpy python3-scipy net-tools unzip vim wget xpra</div></pre></td></tr></table></figure>
<p>安装MuJoCo：</p>
<ol>
<li><p>在<a href="https://www.roboti.us/license.html" target="_blank" rel="external">MuJoCo Website</a>申请30天试用或学生免费证书，这里需要获取Computer id，下载对应系统(Linux)的脚本运行得到id（脚本需要设置运行权限 <code>chmod 777 getid_linux</code>），邮件中会收到license文件mjkey.txt。</p>
</li>
<li><p>下载对应版本的mjpro，<a href="https://www.roboti.us/index.html" target="_blank" rel="external">https://www.roboti.us/index.html</a>，本文选择mjpro131 linux。</p>
</li>
<li><p>解压<code>mjpro131.zip</code>到<code>~/.mujoco/mjpro131</code>，并将license文件放在<code>~/.mujoco/</code>和<code>~/.mujoco/mjpro131/bin</code>目录下。</p>
</li>
<li><p>设置环境变量：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export LD_LIBRARY_PATH=/home/user/.mujoco/mjpro131/bin</div></pre></td></tr></table></figure>
<p> <em>测试是否安装成功：</em></p>
<p> 在<code>~/.mujoco/mjpro131/bin</code>下运行<code>./simulate ../model/humanoid.xml</code>，弹出仿真界面即为安装成功。</p>
<p>安装mujoco-py:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ pip3 install mujoco-py==0.5.7</div></pre></td></tr></table></figure>
<hr>
<p><strong>安装OpenAI gym</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ git clone https://github.com/openai/gym.git</div><div class="line">(tensorflow)$ cd gym</div><div class="line">(tensorflow)$ pip install -e .</div></pre></td></tr></table></figure>
<hr>
<p>测试：通过运行<code>./homework/hw1/demo.bash</code>测试是否安装成功。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhuoyangdu.github.io/2017/11/07/DRL-references-summarizing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhuoyang Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuoyang Du">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/07/DRL-references-summarizing/" itemprop="url">深度强化学习资源整理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-07T20:29:11+08:00">
                2017-11-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>公开课</strong></p>
<ul>
<li><p>David Silver UCL Course 强化学习公开课：<br>  <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="external">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a><br>  读书笔记：<br>  <a href="https://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html" target="_blank" rel="external">https://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html</a></p>
</li>
<li><p>MIT: Deep Learning for Self-driving Cars:<br>  <a href="http://selfdrivingcars.mit.edu/" target="_blank" rel="external">http://selfdrivingcars.mit.edu/</a></p>
</li>
<li><p>BerkeleyX: CS188x_1: Artificial Intelligence:<br>  <a href="https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/20021a0a32d14a31b087db8d4bb582fd/" target="_blank" rel="external">https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/20021a0a32d14a31b087db8d4bb582fd/</a></p>
</li>
</ul>
<hr>
<p><strong>论文</strong></p>
<ul>
<li><p>ICML 2017 tutorial: Deep Reinforcement Learning, Decision Making, and Control<br>  <a href="https://sites.google.com/view/icml17deeprl" target="_blank" rel="external">https://sites.google.com/view/icml17deeprl</a></p>
</li>
<li><p>Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning：<br>  原文：<a href="https://arxiv.org/abs/1705.01196" target="_blank" rel="external">https://arxiv.org/abs/1705.01196</a><br>  机器之心中文翻译：<a href="https://www.jiqizhixin.com/articles/2017-07-27-3" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-07-27-3</a></p>
</li>
</ul>
<hr>
<p><strong>博客</strong></p>
<ul>
<li>Kintoki <a href="http://www.cnblogs.com/jinxulin/tag/Reinforcement%20learning/" target="_blank" rel="external">http://www.cnblogs.com/jinxulin/tag/Reinforcement%20learning/</a></li>
</ul>
<hr>
<p><strong>平台</strong></p>
<ul>
<li>TORCS仿真软件： <a href="http://torcs.sourceforge.net/" target="_blank" rel="external">http://torcs.sourceforge.net/</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Zhuoyang Du" />
            
              <p class="site-author-name" itemprop="name">Zhuoyang Du</p>
              <p class="site-description motion-element" itemprop="description">You only live once.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/zhuoyangdu" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:zydu@zju.edu.cn" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuoyang Du</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
